{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "curious-norfolk",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aware-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cross-federation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "returning-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "given-highland",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/jess/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/jess/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "searching-travel",
   "metadata": {},
   "outputs": [],
   "source": [
    "import linguistica as lxa\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "manual-affairs",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df = pickle.load(open(\"en_df.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "enormous-bermuda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>conversation_id</th>\n",
       "      <th>created_at</th>\n",
       "      <th>date</th>\n",
       "      <th>time</th>\n",
       "      <th>timezone</th>\n",
       "      <th>user_id</th>\n",
       "      <th>username</th>\n",
       "      <th>name</th>\n",
       "      <th>place</th>\n",
       "      <th>...</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>cashtags</th>\n",
       "      <th>link</th>\n",
       "      <th>retweet</th>\n",
       "      <th>quote_url</th>\n",
       "      <th>video</th>\n",
       "      <th>thumbnail</th>\n",
       "      <th>reply_to</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1244776419676303365</td>\n",
       "      <td>1244776419676303365</td>\n",
       "      <td>2020-03-30 18:59:59 CDT</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>18:59:59</td>\n",
       "      <td>-600</td>\n",
       "      <td>1431335726</td>\n",
       "      <td>goreactforasl</td>\n",
       "      <td>GoReact for ASL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['signlanguage', 'deaf']</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/GoReactforASL/status/12447...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Health Workers Study  SignLanguage to Care for...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119129</th>\n",
       "      <td>1244701870326571013</td>\n",
       "      <td>1244701870326571013</td>\n",
       "      <td>2020-03-30 14:03:45 CDT</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>14:03:45</td>\n",
       "      <td>-600</td>\n",
       "      <td>907496840</td>\n",
       "      <td>drmanishshukla_</td>\n",
       "      <td>Dr Manish Shukla</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>['nizamuddin', 'coronaupdatesinindia']</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/drmanishshukla_/status/124...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Corona Virus has spread among some of those wh...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119130</th>\n",
       "      <td>1244701869857034248</td>\n",
       "      <td>1244701449700937734</td>\n",
       "      <td>2020-03-30 14:03:45 CDT</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>14:03:45</td>\n",
       "      <td>-600</td>\n",
       "      <td>1006650378254012418</td>\n",
       "      <td>sheagrandequeen</td>\n",
       "      <td>katie</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/sheagrandequeen/status/124...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'ArianaGrande', 'name': 'Aria...</td>\n",
       "      <td>ArianaGrande bro pls after corona   have this...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119131</th>\n",
       "      <td>1244701869777117188</td>\n",
       "      <td>1244679079443357696</td>\n",
       "      <td>2020-03-30 14:03:45 CDT</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>14:03:45</td>\n",
       "      <td>-600</td>\n",
       "      <td>1100443875531837440</td>\n",
       "      <td>betaal56</td>\n",
       "      <td>Storyteller</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/betaal56/status/1244701869...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[{'screen_name': 'dhanyarajendran', 'name': 'D...</td>\n",
       "      <td>dhanyarajendran Thid Tabliqi thing has become...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119133</th>\n",
       "      <td>1244701868502237190</td>\n",
       "      <td>1244701868502237190</td>\n",
       "      <td>2020-03-30 14:03:45 CDT</td>\n",
       "      <td>2020-03-30</td>\n",
       "      <td>14:03:45</td>\n",
       "      <td>-600</td>\n",
       "      <td>509512845</td>\n",
       "      <td>edward_a123</td>\n",
       "      <td>Eddy</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>https://twitter.com/Edward_A123/status/1244701...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>Yo I can’t wait until self quarantine ends and...</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         id      conversation_id               created_at  \\\n",
       "0       1244776419676303365  1244776419676303365  2020-03-30 18:59:59 CDT   \n",
       "119129  1244701870326571013  1244701870326571013  2020-03-30 14:03:45 CDT   \n",
       "119130  1244701869857034248  1244701449700937734  2020-03-30 14:03:45 CDT   \n",
       "119131  1244701869777117188  1244679079443357696  2020-03-30 14:03:45 CDT   \n",
       "119133  1244701868502237190  1244701868502237190  2020-03-30 14:03:45 CDT   \n",
       "\n",
       "              date      time  timezone              user_id         username  \\\n",
       "0       2020-03-30  18:59:59      -600           1431335726    goreactforasl   \n",
       "119129  2020-03-30  14:03:45      -600            907496840  drmanishshukla_   \n",
       "119130  2020-03-30  14:03:45      -600  1006650378254012418  sheagrandequeen   \n",
       "119131  2020-03-30  14:03:45      -600  1100443875531837440         betaal56   \n",
       "119133  2020-03-30  14:03:45      -600            509512845      edward_a123   \n",
       "\n",
       "                    name place  ...                                hashtags  \\\n",
       "0        GoReact for ASL   NaN  ...                ['signlanguage', 'deaf']   \n",
       "119129  Dr Manish Shukla   NaN  ...  ['nizamuddin', 'coronaupdatesinindia']   \n",
       "119130             katie   NaN  ...                                      []   \n",
       "119131       Storyteller   NaN  ...                                      []   \n",
       "119133              Eddy   NaN  ...                                      []   \n",
       "\n",
       "       cashtags                                               link retweet  \\\n",
       "0            []  https://twitter.com/GoReactforASL/status/12447...   False   \n",
       "119129       []  https://twitter.com/drmanishshukla_/status/124...   False   \n",
       "119130       []  https://twitter.com/sheagrandequeen/status/124...   False   \n",
       "119131       []  https://twitter.com/betaal56/status/1244701869...   False   \n",
       "119133       []  https://twitter.com/Edward_A123/status/1244701...   False   \n",
       "\n",
       "       quote_url  video  thumbnail  \\\n",
       "0            NaN      0        NaN   \n",
       "119129       NaN      0        NaN   \n",
       "119130       NaN      0        NaN   \n",
       "119131       NaN      0        NaN   \n",
       "119133       NaN      0        NaN   \n",
       "\n",
       "                                                 reply_to  \\\n",
       "0                                                      []   \n",
       "119129                                                 []   \n",
       "119130  [{'screen_name': 'ArianaGrande', 'name': 'Aria...   \n",
       "119131  [{'screen_name': 'dhanyarajendran', 'name': 'D...   \n",
       "119133                                                 []   \n",
       "\n",
       "                                                     text lang  \n",
       "0       Health Workers Study  SignLanguage to Care for...   en  \n",
       "119129  Corona Virus has spread among some of those wh...   en  \n",
       "119130   ArianaGrande bro pls after corona   have this...   en  \n",
       "119131   dhanyarajendran Thid Tabliqi thing has become...   en  \n",
       "119133  Yo I can’t wait until self quarantine ends and...   en  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "grand-weapon",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df['tokens'] = en_df['text'].apply(lambda x: word_tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "waiting-services",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [Health, Workers, Study, SignLanguage, to, Car...\n",
       "119129    [Corona, Virus, has, spread, among, some, of, ...\n",
       "119130    [ArianaGrande, bro, pls, after, corona, have, ...\n",
       "119131    [dhanyarajendran, Thid, Tabliqi, thing, has, b...\n",
       "119133    [Yo, I, can, ’, t, wait, until, self, quaranti...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_df['tokens'][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "linear-smile",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words_standard = stopwords.words('english')\n",
    "stop_words_long = open('/Users/jess/workspace/Metis_Projects/Project_4/stopwords/stopwords_long_ranks_nl.txt', 'r', encoding='utf-8').read().split('\\n')\n",
    "stop = set(stop_words_standard + stop_words_long + ['https' , 'http'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "polished-mauritius",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'conversation_id', 'created_at', 'date', 'time', 'timezone',\n",
       "       'user_id', 'username', 'name', 'place', 'tweet', 'language', 'mentions',\n",
       "       'urls', 'photos', 'replies_count', 'retweets_count', 'likes_count',\n",
       "       'hashtags', 'cashtags', 'link', 'retweet', 'quote_url', 'video',\n",
       "       'thumbnail', 'reply_to', 'text', 'lang', 'tokens'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "positive-flight",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df['filtered'] = en_df['tokens'].apply(lambda tweet: [word for word in tweet if word not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "organic-wisdom",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Health',\n",
       " 'Workers',\n",
       " 'Study',\n",
       " 'SignLanguage',\n",
       " 'Care',\n",
       " 'Deaf',\n",
       " 'COVID',\n",
       " 'Patients',\n",
       " 'https',\n",
       " 'imoFWhCBBg']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_df['filtered'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "confident-player",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/jess/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "foster-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns(text):\n",
    "    is_noun = lambda pos: pos[:2] in ('NN', 'NNS')\n",
    "    tokenized = word_tokenize(text)\n",
    "    all_nouns = [word for (word, pos) in pos_tag(tokenized) if is_noun(pos)]\n",
    "    return ' '.join(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "referenced-cover",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df['nouns'] = en_df['text'].apply(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optional-bradford",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ^ last cell run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "laughing-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "en_df['filtered_nouns'] = en_df['nouns'].apply(lambda tweet: [word for word in tweet if word not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "confident-pearl",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokens</th>\n",
       "      <th>filtered</th>\n",
       "      <th>nouns</th>\n",
       "      <th>filtered_nouns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Health, Workers, Study, SignLanguage, to, Car...</td>\n",
       "      <td>[Health, Workers, Study, SignLanguage, Care, D...</td>\n",
       "      <td>Health Workers Study SignLanguage Care Deaf CO...</td>\n",
       "      <td>[H,  , W,  , S,  , S, L,  , C,  , D,  , C, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119129</th>\n",
       "      <td>[Corona, Virus, has, spread, among, some, of, ...</td>\n",
       "      <td>[Corona, Virus, spread, attended, religious, p...</td>\n",
       "      <td>Corona Virus prayer meeting March Markaz Nizam...</td>\n",
       "      <td>[C,  , V,  ,  ,  , M,  , M,  , N,  ,  , T,  , ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119130</th>\n",
       "      <td>[ArianaGrande, bro, pls, after, corona, have, ...</td>\n",
       "      <td>[ArianaGrande, bro, pls, corona, pls, thx]</td>\n",
       "      <td>ArianaGrande bro pls corona look pls thx</td>\n",
       "      <td>[A, G,  ,  ,  ,  ,  ,  ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119131</th>\n",
       "      <td>[dhanyarajendran, Thid, Tabliqi, thing, has, b...</td>\n",
       "      <td>[dhanyarajendran, Thid, Tabliqi, thing, bigges...</td>\n",
       "      <td>dhanyarajendran Thid Tabliqi thing community s...</td>\n",
       "      <td>[ , T,  , T,  ,  ,  ,  ,  ,  ,  ,  ,  , C,  ,  ]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119133</th>\n",
       "      <td>[Yo, I, can, ’, t, wait, until, self, quaranti...</td>\n",
       "      <td>[Yo, I, ’, wait, quarantine, ends, corona, virus]</td>\n",
       "      <td>Yo wait ends corona virus</td>\n",
       "      <td>[Y,  ,  ,  ,  ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   tokens  \\\n",
       "0       [Health, Workers, Study, SignLanguage, to, Car...   \n",
       "119129  [Corona, Virus, has, spread, among, some, of, ...   \n",
       "119130  [ArianaGrande, bro, pls, after, corona, have, ...   \n",
       "119131  [dhanyarajendran, Thid, Tabliqi, thing, has, b...   \n",
       "119133  [Yo, I, can, ’, t, wait, until, self, quaranti...   \n",
       "\n",
       "                                                 filtered  \\\n",
       "0       [Health, Workers, Study, SignLanguage, Care, D...   \n",
       "119129  [Corona, Virus, spread, attended, religious, p...   \n",
       "119130         [ArianaGrande, bro, pls, corona, pls, thx]   \n",
       "119131  [dhanyarajendran, Thid, Tabliqi, thing, bigges...   \n",
       "119133  [Yo, I, ’, wait, quarantine, ends, corona, virus]   \n",
       "\n",
       "                                                    nouns  \\\n",
       "0       Health Workers Study SignLanguage Care Deaf CO...   \n",
       "119129  Corona Virus prayer meeting March Markaz Nizam...   \n",
       "119130           ArianaGrande bro pls corona look pls thx   \n",
       "119131  dhanyarajendran Thid Tabliqi thing community s...   \n",
       "119133                          Yo wait ends corona virus   \n",
       "\n",
       "                                           filtered_nouns  \n",
       "0       [H,  , W,  , S,  , S, L,  , C,  , D,  , C, O, ...  \n",
       "119129  [C,  , V,  ,  ,  , M,  , M,  , N,  ,  , T,  , ...  \n",
       "119130                           [A, G,  ,  ,  ,  ,  ,  ]  \n",
       "119131   [ , T,  , T,  ,  ,  ,  ,  ,  ,  ,  ,  , C,  ,  ]  \n",
       "119133                                    [Y,  ,  ,  ,  ]  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_df[['tokens', 'filtered', 'nouns','filtered_nouns']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "extraordinary-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(en_df, open('en_df.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "processed-footwear",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2,\n",
    "                            stop_words=my_stop_words)\n",
    "tfidf_en = vectorizer.fit_transform(en_df['filtered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "induced-valve",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-grass",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf_en, open('tfidf_en_sparse.pkl', 'wb'))\n",
    "pickle.dump(vectorizer, open('en_vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-citation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "handled-fourth",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(df):\n",
    "    data_dict = dict()\n",
    "\n",
    "    tokens = []\n",
    "    for row in en_df.index.tolist():\n",
    "        tokens += en_df.loc[row, 'filtered'].split()\n",
    "\n",
    "    lxa_object = lxa.from_corpus(tokens)\n",
    "\n",
    "    unigrams = lxa_object.word_unigram_counter()\n",
    "    bigrams = lxa_object.word_bigram_counter()\n",
    "    trigrams = lxa_object.word_trigram_counter()\n",
    "\n",
    "    df_unigrams = pd.DataFrame.from_dict(unigrams, orient='index') \\\n",
    "    .reset_index()\n",
    "\n",
    "\n",
    "    df_bigrams = pd.DataFrame.from_dict(bigrams, orient='index') \\\n",
    "    .reset_index()\n",
    "\n",
    "\n",
    "    df_trigrams = pd.DataFrame.from_dict(trigrams, orient='index') \\\n",
    "    .reset_index()\n",
    "\n",
    "    df_unigrams.columns = ['N-Gram','Frequency']\n",
    "    df_bigrams.columns = ['N-Gram','Frequency']\n",
    "    df_trigrams.columns = ['N-Gram', 'Frequency']\n",
    "    \n",
    "    df_bigrams[['word1', 'word2']] = df_bigrams['N-Gram'].apply(pd.Series)\n",
    "    df_trigrams[['word1', 'word2', 'word3']] = df_trigrams['N-Gram'].apply(pd.Series)\n",
    "\n",
    "    data_dict['Unigrams'] = df_unigrams\n",
    "    data_dict['Bigrams'] = df_bigrams\n",
    "    data_dict['Trigrams'] = df_trigrams\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-mouse",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unigrams.columns = ['N-Gram','Frequency']\n",
    "df_bigrams.columns = ['N-Gram','Frequency']\n",
    "df_trigrams.columns = ['N-Gram', 'Frequency']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-monitoring",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bigrams[['word1', 'word2']] = df_bigrams['N-Gram'].apply(pd.Series)\n",
    "df_trigrams[['word1', 'word2', 'word3']] = df_trigrams['N-Gram'].apply(pd.Series)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-memorabilia",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict['Unigrams'] = df_unigrams\n",
    "data_dict['Bigrams'] = df_bigrams\n",
    "data_dict['Trigrams'] = df_trigrams\n",
    "\n",
    "data_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medium-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(chart_data, num_grams=30, ascending=False, font_scale=1, \n",
    "        chart_scale='poster', chart_size=15, save_to_file=True, title=\"\"):\n",
    "   \n",
    "    chart_data = chart_data.head(num_grams)\\\n",
    "    .reset_index().sort_values(by='Frequency', ascending=ascending).copy()\n",
    "    \n",
    "    sns.set_context(chart_scale, font_scale=font_scale)\n",
    "    \n",
    "    sns_plot = sns.catplot(x=\"Frequency\", y='N-Gram', data=chart_data, kind=\"bar\",\\\n",
    "                           size=chart_size, palette=\"flare\");\n",
    "    plt.title(title)\n",
    "    plt.show();\n",
    "    if save_to_file is not False:\n",
    "        sns_plot.savefig(table_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "framed-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Unigrams', 'Bigrams', 'Trigrams']\n",
    "\n",
    "for title in titles:\n",
    "    viz(data_dict[title].sort_values(by='Frequency', ascending=False), num_grams=10, \\\n",
    "       chart_scale='poster', chart_size=9, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coated-emergency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "growing-basin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "difficult-friday",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-cycling",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "indonesian-school",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-variety",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "younger-familiar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ahead-dinner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aging-gardening",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "helpful-communication",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-testing",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hybrid-footwear",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alternative-chapel",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-medium",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "negative-regard",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dress-india",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-preservation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-interference",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-gazette",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recent-anchor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-amateur",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "integral-consciousness",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-month",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ruled-creature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coastal-longer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "logical-grammar",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-accountability",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-proposition",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-pierre",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-knitting",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-disaster",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "approved-silver",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinated-elements",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extreme-world",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
