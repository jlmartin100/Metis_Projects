{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "mental-disney",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "respected-baking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hispanic-harbor",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from gensim import corpora, models, similarities, matutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "visible-insulin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import linguistica as lxa\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "muslim-thinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_df = pickle.load(open(\"ar_df.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "proved-commitment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.tokenizers.word import simple_word_tokenize\n",
    "from camel_tools.utils.normalize import normalize_unicode\n",
    "from camel_tools.dialectid import DialectIdentifier, label_to_region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "neural-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "from camel_tools.tagger.default import DefaultTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "eligible-revision",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_ar_text(text):\n",
    "    mle_msa = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "    tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, \n",
    "                                       scheme='bwtok',\n",
    "                                      split=True)\n",
    "    \n",
    "    norm_text = normalize_unicode(text)\n",
    "    clean_text = norm_text.strip()\n",
    "    simple_tokens = simple_word_tokenize(clean_text)\n",
    "    tokens = tokenizer.tokenize(simple_tokens)\n",
    "    tokens_clean = []\n",
    "    for token in tokens:\n",
    "        if \"+\" not in token:\n",
    "            tokens_clean.append(token)\n",
    "    return tokens_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "toxic-throw",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text      كورونا  كوفيد    صحة  الرياض  تثقيف  د ولاء ا...\n",
       "tweet    #كورونا #كوفيد_19 #صحة #الرياض #تثقيف #د_ولاء_...\n",
       "Name: 117750, dtype: object"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_df[['text', 'tweet']].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "disturbed-amber",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "121"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ar_df['tweet'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "temporal-identification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ar_df['text'][0].apply(clean_ar_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "fewer-input",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ar_df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "imposed-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "token = normalize_unicode(ar_df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "normal-donor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'كورونا  كوفيد    صحة  الرياض  تثقيف  د ولاء الرفاس  السعوديون يد واحدة ضد كورونا  السعودية  خليكم في البيت  خليك بالبيت'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "artificial-omega",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = simple_word_tokenize(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "latin-architecture",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_msa = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, \n",
    "                                       scheme='bwtok',\n",
    "                                      split=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "satellite-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "spoken-genetics",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['كور',\n",
       " '+و',\n",
       " '+نا',\n",
       " 'كوفيد',\n",
       " 'صح',\n",
       " '+ة',\n",
       " 'ال+',\n",
       " 'رياض',\n",
       " 'تثقيف',\n",
       " 'د',\n",
       " 'ولاء',\n",
       " 'ال+',\n",
       " 'رفاس',\n",
       " 'ال+',\n",
       " 'سعودي',\n",
       " '+ون',\n",
       " 'يد',\n",
       " 'واحد',\n",
       " '+ة',\n",
       " 'ضد',\n",
       " 'كور',\n",
       " '+و',\n",
       " '+نا',\n",
       " 'ال+',\n",
       " 'سعودي',\n",
       " '+ة',\n",
       " 'خلي',\n",
       " '+كم',\n",
       " 'في',\n",
       " 'ال+',\n",
       " 'بيت',\n",
       " 'خليك',\n",
       " 'ب+',\n",
       " 'ال+',\n",
       " 'بيت']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "stretch-encounter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['كور',\n",
       " 'كوفيد',\n",
       " 'صح',\n",
       " 'رياض',\n",
       " 'تثقيف',\n",
       " 'د',\n",
       " 'ولاء',\n",
       " 'رفاس',\n",
       " 'سعودي',\n",
       " 'يد',\n",
       " 'واحد',\n",
       " 'ضد',\n",
       " 'كور',\n",
       " 'سعودي',\n",
       " 'خلي',\n",
       " 'في',\n",
       " 'بيت',\n",
       " 'خليك',\n",
       " 'بيت']"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_ar_text(ar_df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assured-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prostate-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "choice-twelve",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " كورونا  كوفيد    صحة  الرياض  تثقيف  د ولاء الرفاس  السعوديون يد واحدة ضد كورونا  السعودية  خليكم في البيت  خليك بالبيت\n"
     ]
    }
   ],
   "source": [
    "print(ar_df['text'].iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "falling-accordance",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-156-0eec56096cb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mar_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tokens'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_ar_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4132\u001b[0m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4133\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4134\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4135\u001b[0m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   5872\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5873\u001b[0m             \u001b[0;31m# else, only a single dtype is given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5874\u001b[0;31m             \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5875\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    629\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"raise\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    630\u001b[0m     ) -> \"BlockManager\":\n\u001b[0;32m--> 631\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"astype\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m     def convert(\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m                     \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mignore_failures\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_dtype_equal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/metis/lib/python3.8/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, deep)\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 756\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    757\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_block_same_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ar_df['tokens'] = ar_df['text'].apply(clean_ar_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recognized-italy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check tokens ^ this cell run recently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classified-richardson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ar_df[['tweet', 'text', 'tokens']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-capture",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_df.lang.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "vocal-finder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def id_dialect(text):\n",
    "#     DID = DialectIdentifier.pretrained()\n",
    "#     predictions = DID.predict(text)\n",
    "# #     region_id = label_to_region(DID.predict(text))\n",
    "#     top_dialect = [p.top for p in predictions]\n",
    "#     return top_dialect\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "peripheral-transsexual",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = ar_df['clean_text'][0:5].apply(id_dialect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grand-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_o_s(text):\n",
    "    mled = MLEDisambiguator.pretrained()\n",
    "    tagger = DefaultTagger(mled, 'pos')\n",
    "    pos_tagged = tagger.tag(text.split())\n",
    "    return zip(text, pos_tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-player",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_df['POS'] = ar_df['tokens'].apply(p_o_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nouns(word, pos):\n",
    "    is_noun = lambda pos: pos[:1] == 'noun'\n",
    "    all_nouns = [word for (word, pos) in text if is_noun(text)]\n",
    "    return all_nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "freelance-vinyl",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_df['nouns'] = ar_df['POS'].apply(lambda (word, pos): nouns(word, pos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "planned-russia",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_df[['tokens','nouns', 'POS']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "optional-magnet",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ar_df, open('ar_df.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "played-shame",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "pending-newman",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_url = \"/Users/jess/workspace/Metis_Projects/Project_5/camel_tools/arabic-stop-words/list.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "beautiful-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stopwords(file_location):\n",
    "    lines = []\n",
    "    file = open(file_location)\n",
    "    lines = file.read().split('\\n')\n",
    "        \n",
    "    file.close()\n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "accompanied-jordan",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = get_stopwords(stopword_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "impaired-election",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['،', 'ء', 'ءَ', 'آ', 'آب']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "continued-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "# my_stop_words = text.ENGLISH_STOP_WORDS.union(stopwords)\n",
    "my_stop_words = set(text.ENGLISH_STOP_WORDS + stopwords + 'http' + 'https')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "appointed-durham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1069"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(my_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norman-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_df['filtered'] = en_df['tokens'].apply(lambda tweet: [word for word in tweet if word not in my_stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "consecutive-latter",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(min_df=2, \n",
    "                             stop_words=my_stop_words)\n",
    "tfidf1 = vectorizer.fit_transform(ar_df['filtered'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "another-serial",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "documentary-kernel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "360907"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "thermal-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "idf = vectorizer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "white-child",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "overall-veteran",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([14.34159288, 14.34159288, 13.42530215, 10.9403955 , 14.34159288])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ready-turtle",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_tfidf_df = pd.DataFrame(tfidf1.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "passing-realtor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>2021</th>\n",
       "      <th>24</th>\n",
       "      <th>30</th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaa</th>\n",
       "      <th>aaaaaabbbbk</th>\n",
       "      <th>aaaaaoxxxxx</th>\n",
       "      <th>aaaagroup</th>\n",
       "      <th>aaaalmarri</th>\n",
       "      <th>...</th>\n",
       "      <th>제이홉</th>\n",
       "      <th>조건없이</th>\n",
       "      <th>종대야</th>\n",
       "      <th>주헌</th>\n",
       "      <th>준면이</th>\n",
       "      <th>지민</th>\n",
       "      <th>지민아고마워사랑해</th>\n",
       "      <th>초이스</th>\n",
       "      <th>축하해</th>\n",
       "      <th>행복해</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 360907 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   2021   24   30   aa  aaa  aaaa  aaaaaabbbbk  aaaaaoxxxxx  aaaagroup  \\\n",
       "0   0.0  0.0  0.0  0.0  0.0   0.0          0.0          0.0        0.0   \n",
       "1   0.0  0.0  0.0  0.0  0.0   0.0          0.0          0.0        0.0   \n",
       "2   0.0  0.0  0.0  0.0  0.0   0.0          0.0          0.0        0.0   \n",
       "3   0.0  0.0  0.0  0.0  0.0   0.0          0.0          0.0        0.0   \n",
       "4   0.0  0.0  0.0  0.0  0.0   0.0          0.0          0.0        0.0   \n",
       "\n",
       "   aaaalmarri  ...  제이홉  조건없이  종대야   주헌  준면이   지민  지민아고마워사랑해  초이스  축하해  행복해  \n",
       "0         0.0  ...  0.0   0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0  \n",
       "1         0.0  ...  0.0   0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0  \n",
       "2         0.0  ...  0.0   0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0  \n",
       "3         0.0  ...  0.0   0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0  \n",
       "4         0.0  ...  0.0   0.0  0.0  0.0  0.0  0.0        0.0  0.0  0.0  0.0  \n",
       "\n",
       "[5 rows x 360907 columns]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ar_tfidf_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "narrative-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(tfidf1, open(\"tfidf1_sparse.pkl\", 'wb'))\n",
    "pickle.dump(vectorizer, open('ar_vectorizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-forth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "another-worker",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams(df):\n",
    "    data_dict = dict()\n",
    "\n",
    "    tokens = []\n",
    "    for row in en_df.index.tolist():\n",
    "        tokens += en_df.loc[row, 'filtered'].split()\n",
    "\n",
    "    lxa_object = lxa.from_corpus(tokens)\n",
    "\n",
    "    unigrams = lxa_object.word_unigram_counter()\n",
    "    bigrams = lxa_object.word_bigram_counter()\n",
    "    trigrams = lxa_object.word_trigram_counter()\n",
    "\n",
    "    df_unigrams = pd.DataFrame.from_dict(unigrams, orient='index') \\\n",
    "    .reset_index()\n",
    "\n",
    "\n",
    "    df_bigrams = pd.DataFrame.from_dict(bigrams, orient='index') \\\n",
    "    .reset_index()\n",
    "\n",
    "\n",
    "    df_trigrams = pd.DataFrame.from_dict(trigrams, orient='index') \\\n",
    "    .reset_index()\n",
    "\n",
    "    df_unigrams.columns = ['N-Gram','Frequency']\n",
    "    df_bigrams.columns = ['N-Gram','Frequency']\n",
    "    df_trigrams.columns = ['N-Gram', 'Frequency']\n",
    "    \n",
    "    df_bigrams[['word1', 'word2']] = df_bigrams['N-Gram'].apply(pd.Series)\n",
    "    df_trigrams[['word1', 'word2', 'word3']] = df_trigrams['N-Gram'].apply(pd.Series)\n",
    "\n",
    "    data_dict['Unigrams'] = df_unigrams\n",
    "    data_dict['Bigrams'] = df_bigrams\n",
    "    data_dict['Trigrams'] = df_trigrams\n",
    "    \n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-swedish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viz(chart_data, num_grams=30, ascending=False, font_scale=1, \n",
    "        chart_scale='poster', chart_size=15, save_to_file=True, title=\"\"):\n",
    "   \n",
    "    chart_data = chart_data.head(num_grams)\\\n",
    "    .reset_index().sort_values(by='Frequency', ascending=ascending).copy()\n",
    "    \n",
    "    sns.set_context(chart_scale, font_scale=font_scale)\n",
    "    \n",
    "    sns_plot = sns.catplot(x=\"Frequency\", y='N-Gram', data=chart_data, kind=\"bar\",\\\n",
    "                           size=chart_size, palette=\"flare\");\n",
    "    plt.title(title)\n",
    "    plt.show();\n",
    "    if save_to_file is not False:\n",
    "        sns_plot.savefig(table_name)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = ['Unigrams', 'Bigrams', 'Trigrams']\n",
    "\n",
    "for title in titles:\n",
    "    viz(data_dict[title].sort_values(by='Frequency', ascending=False), num_grams=10, \\\n",
    "       chart_scale='poster', chart_size=9, title=title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "duplicate-actor",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-sharing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
